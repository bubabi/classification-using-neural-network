{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hacettepe University Computer Science Department\n",
    "## Assignment 3\n",
    "\n",
    "***\n",
    "\n",
    "Name and Surname : _Burak Emre Ozer_\n",
    "\n",
    "Identity Number : _21527266_\n",
    "\n",
    "Course : _Introduction to Machine Learning Lab. (BBM409)_\n",
    "\n",
    "Due : _20/12/2018_\n",
    "\n",
    "Advisors: _Dr. Aykut Erdem, T.A. Necva Bölücü_\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 1: Theory Questions\n",
    "\n",
    "***\n",
    "\n",
    "### 1. What are differences between logistic regression and linear regression?\n",
    "\n",
    "The difference between **linear regression** and **logistic regression** is that linear regression is used to predict a _**continuous value**_ while logistic regression is used to predict a _**discrete value**_.  In brief, linear regression is used for regression while logistic regression is used for classification.\n",
    "\n",
    "### 2. What are differences between logistic regression and naive bayes methods?\n",
    "\n",
    "Logistic Regression makes a prediction for the probability using a direct functional form where as Naive Bayes figures out how the data was generated given the results. Naive Bayes also assumes that the features are conditionally independent. \n",
    "\n",
    "### 3. Which of the following statements are true?\n",
    "\n",
    "* _False_\n",
    "* **True**\n",
    "* _False_\n",
    "* **True**\n",
    "\n",
    "### 4. How to decide the number of hidden layers and nodes in a hidden layer?\n",
    "\n",
    "**Hidden Layer**\n",
    "\n",
    "* Two or fewer hidden layers will often suffice with simple data sets. However, with complex datasets involving time-series or computer vision, additional layers can be helpful.\n",
    "\n",
    "**Nodes in a hidden layer**\n",
    "\n",
    "* The number of hidden neurons should be between the size of the input layer and the size of the output layer.\n",
    "* The number of hidden neurons should be 2/3 the size of the input layer, plus the size of the output layer.\n",
    "* The number of hidden neurons should be less than twice the size of the input layer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 2: Classification of Flowers using Neural Network\n",
    "\n",
    "***\n",
    "\n",
    "### Aim\n",
    "\n",
    "We will implement a simple single layer neural network and multilayer neural network\n",
    "architecture to classify flowers into 5 classses which daisy, tulip, rose, sunflower, dandelion.\n",
    "\n",
    "### Data source\n",
    "\n",
    "<img src=\"images/dataset.png\">\n",
    "\n",
    "\n",
    "This dataset contains 4242 images of flowers. The data collection is based on the data flickr, google images, yandex images. \n",
    "\n",
    "The pictures are divided into five classes: daisy, tulip, rose, sunflower, dandelion. For each class there are about 800 photos. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.  Single Layer Neural Network\n",
    "\n",
    "The architecture of this single-layer neural network has 768 inputs and 5 outputs. \n",
    "\n",
    "I used the **softmax** function which is uses for the multiclass logistic regression. Softmax is kind of Multi Class Sigmoid basically getting a probability of each class. I also used **cross-entropy** to calculate the loss. \n",
    "\n",
    "Let's overview of the core point of the code:\n",
    "\n",
    "```python\n",
    "    def train(self):\n",
    "        X = self.X\n",
    "        for i in np.arange(0, self.epoch):\n",
    "            epoch_loss_values = []\n",
    "            # train the data using mini-batch\n",
    "            for current_batch in np.arange(0, X.shape[0], self.batch_size):\n",
    "                bX = X[current_batch:current_batch + self.batch_size]\n",
    "                batch_labels = self.y[current_batch:current_batch + self.batch_size]\n",
    "\n",
    "                outputs = self.feedforward(bX)\n",
    "                probs = self.softmax(outputs) # getting a probability of each class\n",
    "\n",
    "                loss = self.cross_entropy_loss(bX, batch_labels, probs) # calc. the loss using cross-entropy\n",
    "                epoch_loss_values.append(loss)\n",
    "\n",
    "                self.backpropagation(bX, batch_labels, probs) # minimize the loss func. using gradient descent\n",
    "            self.loss.append(np.mean(epoch_loss_values))\n",
    "\n",
    "        model = [self.weights, self.b]\n",
    "        np.save(\"slnn_model\", model) # save the model current directory\n",
    "```\n",
    "\n",
    "## 1.1 Software Usage\n",
    "\n",
    "* _firstly, run train.py to train the model_:\n",
    "```python\n",
    "python3 train.py -data_path path/to/data -e epoch number -a learning rate -b batch size```\n",
    "\n",
    "* _then run test.py to calculate the results_:\n",
    "```python\n",
    "python3 test.py -data_path path/to/data -model_path path/to/model```\n",
    "\n",
    "## 1.2 Visualization of parameters\n",
    "\n",
    "<img src=\"images/paramsr.png\">\n",
    "\n",
    "If we pay attention to the sunflower and the tulip, the model looks well trained. But the accuracy rates overlap with this situation.\n",
    "\n",
    "## 1.3 Analyzing the experiments and their effects\n",
    "\n",
    "#### 1.3.1 Activation function\n",
    "\n",
    "<img src=\"images/act_func.png\" width=\"400px\">\n",
    "\n",
    "Different activation functions, in fact, do have different properties. I use softmax for the output layer and cross entropy as a cost function. Also I used sigmoid or tanh with squared error as a cost function. As seen from the plot, the most successful activation function was reLU for this problem. \n",
    "\n",
    "#### 1.3.2 Epoch number effect\n",
    "\n",
    "As the number of epochs increases, more number of times the weight are changed in the neural network and the curve goes from underfitting to optimal to overfitting curve.\n",
    "\n",
    "|epoch| learning rate |batch size| accuracy |\n",
    "|:---:|:---:|:---:|:---:|\n",
    "|50|0.2|128|0.32|\n",
    "|100|0.2|128|0.35|\n",
    "|250|0.2|128|0.32|\n",
    "|500|0.2|128|0.31|\n",
    "\n",
    "| &nbsp;|&nbsp; |\n",
    ":-------------------------:|:-------------------------:\n",
    "<img src=\"images/e50alp002.png\" width=\"250px\">  |  <img src=\"images/e100alp002.png\" width=\"250px\"> |\n",
    "<img src=\"images/e250alp002.png\" width=\"250px\">|  <img src=\"images/e500alp002.png\" width=\"250px\">\n",
    "\n",
    "***\n",
    "\n",
    "#### 1.3.3 Learning rate effect\n",
    "\n",
    "<img src=\"images/alpha.png\" align=\"right\" width=\"200px\">\n",
    "\n",
    "The learning rate affects how quickly our model can converge to a local minima. If it is too big, may not converge; if it is too small, it occurs very slow convergence. Thus getting it right from the get go would mean lesser time for us to train the model.\n",
    "\n",
    "|epoch| learning rate |batch size| accuracy |\n",
    "|:---:|:---:|:---:|:---:|\n",
    "|100|0.005|128|0.32|\n",
    "|100|0.01|128|0.33|\n",
    "|100|0.015|128|0.34|\n",
    "|100|0.02|128|0.35|\n",
    "\n",
    "\n",
    "| &nbsp;|&nbsp; |\n",
    ":-------------------------:|:-------------------------\n",
    "<img src=\"images/a0005.png\" width=\"250px\">  |  <img src=\"images/a001.png\" width=\"250px\"> |  \n",
    "<img src=\"images/a0015.png\" width=\"250px\">|  <img src=\"images/a002.png\" width=\"250px\">\n",
    "\n",
    "***\n",
    "\n",
    "#### 1.3.4 Batch size effect\n",
    "\n",
    "Total number of training examples present in a single batch. With a small batch size, the gradients are only a very rough approximation of the true gradients.\n",
    "\n",
    "|epoch| learning rate |batch size| accuracy |\n",
    "|:---:|:---:|:---:|:---:|\n",
    "|250|0.015|16|0.31|\n",
    "|250|0.015|32|0.32|\n",
    "|250|0.015|64|0.31|\n",
    "|250|0.015|128|0.34|\n",
    "\n",
    "\n",
    "| &nbsp;|&nbsp; |\n",
    ":-------------------------:|:-------------------------:\n",
    "<img src=\"images/b16.png\" width=\"250px\">  |  <img src=\"images/b32.png\" width=\"250px\"> |\n",
    "<img src=\"images/b64.png\" width=\"250px\">|  <img src=\"images/b128.png\" width=\"250px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.  Multi Layer Neural Network\n",
    "\n",
    "The architecture of this multi-layer neural network has 1 hidden layer and output layer. \n",
    "Hidden layer node size: 100\n",
    "\n",
    "\n",
    "Let's overview of the core point of the code:\n",
    "\n",
    "```python\n",
    "    class MultiLayer:\n",
    "        def __init__(self, x, y, epoch, alpha, batch_size):\n",
    "            self.W = np.random.rand(self.X.shape[1], 50) / 500\n",
    "            self.W2 = np.random.rand(50, 5) / 500\n",
    "            self.b = np.zeros((1, 50))\n",
    "            self.b2 = np.zeros((1, 5))\n",
    "            \n",
    "        def feedforward(self, bX):\n",
    "            # using reLu activation func.\n",
    "            hidden_layer = np.maximum(0, np.dot(bX, self.W) + self.b)  \n",
    "            scores = np.dot(hidden_layer, self.W2) + self.b2\n",
    "            return hidden_layer, scores\n",
    "            \n",
    "        def backpropagation(self, bX, bY, probs, hidden_layer):\n",
    "            #calculate gradient descent using by derivative of softmax\n",
    "            d_outputs = probs\n",
    "            d_outputs[range(bX.shape[0]), bY] -= 1\n",
    "            d_outputs /= bX.shape[0]\n",
    "\n",
    "            dW2 = np.dot(hidden_layer.T, d_outputs)\n",
    "            db2 = np.sum(d_outputs, axis=0, keepdims=True)\n",
    "            #backprop. to hidden layer\n",
    "            d_hidden = np.dot(d_outputs, self.W2.T)\n",
    "            # then use derivative of the reLu activation func.\n",
    "            d_hidden[hidden_layer <= 0] = 0\n",
    "            dW = np.dot(bX.T, d_hidden)\n",
    "            db = np.sum(dhidden, axis=0, keepdims=True)\n",
    "            # updating the parameters\n",
    "            self.W += -self.alpha * dW\n",
    "            self.b += -self.alpha * db\n",
    "            self.W2 += -self.alpha * dW2\n",
    "            self.b2 += -self.alpha * db2\n",
    "\n",
    "```\n",
    "\n",
    "## 1.1 Software Usage\n",
    "\n",
    "Please the uncomment the multilayer part and comment single layer part in train.py if you want to use multi layer neural network.\n",
    "\n",
    "* _firstly, run train.py to train the model_:\n",
    "```python\n",
    "python3 train.py -data_path path/to/data -e epoch number -a learning rate -b batch size```\n",
    "\n",
    "* _then run test.py to calculate the results_:\n",
    "```python\n",
    "python3 test.py -data_path path/to/data -model_path path/to/model```\n",
    "\n",
    "\n",
    "## 1.2 Analyzing the experiments and their effects\n",
    "\n",
    "Although I tried different parameters in the multi layer section, I couldn't get the results I expected. So I could not make the necessary improvements. I couldn't find out what I did wrong. :(\n",
    "\n",
    "<img src=\"images/multi_results.png\" width=\"400px\">\n",
    "\n",
    "#### 1.2.1 Learning rate effect\n",
    "\n",
    "\n",
    "|epoch| learning rate |batch size| accuracy |\n",
    "|:---:|:---:|:---:|:---:|\n",
    "|300|0.005|128|0.33|\n",
    "|300|0.01|128|0.36|\n",
    "|300|0.015|128|0.35|\n",
    "|300|0.02|128|0.34|\n",
    "\n",
    "\n",
    "| &nbsp;|&nbsp; |&nbsp; |&nbsp; |\n",
    ":-------------------------:|:-------------------------|:-------------------------|:-------------------------\n",
    "<img src=\"images/ma0005.png\" width=\"250px\">  |  <img src=\"images/ma001.png\" width=\"250px\"> |  <img src=\"images/ma0015.png\" width=\"250px\">|  <img src=\"images/ma002.png\" width=\"250px\">\n",
    "\n",
    "\n",
    "#### 1.3.4 Batch size effect\n",
    "\n",
    "\n",
    "| &nbsp;|&nbsp; |&nbsp; |&nbsp; |\n",
    ":-------------------------:|:-------------------------:|:-------------------------:|:-------------------------:\n",
    "<img src=\"images/mb16.png\" width=\"250px\">  |  <img src=\"images/mb32.png\" width=\"250px\"> | <img src=\"images/mb64.png\" width=\"250px\"> |  <img src=\"images/mb128.png\" width=\"250px\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources\n",
    "\n",
    "https://medium.com/@michaeldelsole/a-single-layer-artificial-neural-network-in-20-lines-of-python-ae34b47e5fef\n",
    "\n",
    "https://enlight.nyc/projects/neural-network/\n",
    "\n",
    "https://medium.com/analytics-vidhya/neural-networks-for-digits-recognition-e11d9dff00d5\n",
    "\n",
    "http://cs231n.github.io/neural-networks-case-study/#grad\n",
    "\n",
    "https://github.com/mnielsen/neural-networks-and-deep-learning/blob/master/src/network.py\n",
    "\n",
    "https://medium.freecodecamp.org/building-a-3-layer-neural-network-from-scratch-99239c4af5d3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
